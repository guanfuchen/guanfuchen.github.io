<!DOCTYPE html>
<html class="no-js" lang="en-US" prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#">
<head>
    <meta charset="utf-8">

    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="description" content="">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="keywords" content="">

 
<meta property="og:type" content="article"/>
<meta property="og:description" content=""/>
<meta property="og:title" content="CVPR2018_相关文章文类 : spf13.com"/>
<meta property="og:site_name" content="spf13 is Steve Francia"/>
<meta property="og:image" content="" />
<meta property="og:image:type" content="image/jpeg" />
<meta property="og:image:width" content="" />
<meta property="og:image:height" content="" />
<meta property="og:url" content="https://guanfuchen.github.io/post/markdown_blog_ws/markdown_blog_2019_04/cvpr2018_%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%E6%96%87%E7%B1%BB/">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-04-24"/>
<meta property="article:modified_time" content="2019-04-24"/>





<meta name="twitter:card" content="summary">

<meta name="twitter:site" content="@spf13">
<meta name="twitter:title" content="CVPR2018_相关文章文类 : spf13.com">
<meta name="twitter:creator" content="@spf13">
<meta name="twitter:description" content="">
<meta name="twitter:image:src" content="">
<meta name="twitter:domain" content="spf13.com">



    <base href="https://guanfuchen.github.io">
    <title> CVPR2018_相关文章文类 - spf13.com </title>
    <link rel="canonical" href="https://guanfuchen.github.io/post/markdown_blog_ws/markdown_blog_2019_04/cvpr2018_%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%E6%96%87%E7%B1%BB/">
    

    <link href='https://fonts.lug.ustc.edu.cn/css?family=Fjalla+One|Open+Sans:300' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="https://guanfuchen.github.io/static/css/style.css">

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/apple-touch-icon.png" />
</head>

<body lang="en" itemscope itemtype="http://schema.org/Article">
<header id="header">
    
      
    
    <div align="center">
        <a href="https://guanfuchen.github.io">
        <img src="https://guanfuchen.github.io/media/avatar.png">
        </a>
    </div>
    <div align="center">guanfuchen</div>
    <nav id="nav">
            <ul id="mainnav">
            <li>
                <a href="https://guanfuchen.github.io/post/">
                <span class="icon"> <i aria-hidden="true" class="icon-quill"></i></span>
                <span> 博客 </span>
            </a>
            </li>
            
            
                
                
            
            
            
            
                
                
            
            
            <li>
            <a href="http://github.com/guanfuchen">
                <span class="icon"> <i aria-hidden="true" class="icon-13"></i></span>
                <span> 关于 </span>
            </a>
            </li>
            <li>
                <a href="https://guanfuchen.github.io/resume.pdf">
                    <span class="icon"> <i aria-hidden="true" class="icon-console"></i></span>
                    <span> 简历 </span>
                </a>
            </li>
        </ul>

    
    </nav>
</header>



<section id="main">
  <h1 itemprop="name" id="title">CVPR2018_相关文章文类</h1>
  <div>
        <article itemprop="articleBody" id="content">
           

<hr />

<h1 id="关键点检测">关键点检测</h1>

<ul>
<li>Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer</li>
</ul>

<p>|Abstract|
| - |
| Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset, and we achieve state-of-the-art human parsing results. Our approach is general and can be readily extended to other object/animal parsing task assuming that their anatomical similarity can be annotated by keypoints. The proposed model and accompanying source code will be made publicly available. |</p>

<ul>
<li>Detect-and-Track: Efficient Pose Estimation in Videos</li>
</ul>

<p>|Abstract|
| - |
| This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection and video understanding. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2% on the validation and 51.8% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge. |</p>

<ul>
<li>Human Pose Estimation With Parsing Induced Learner</li>
</ul>

<p>|Abstract|
| - |
| Human pose estimation still faces various difficulties in challenging scenarios. Human parsing, as a closely related task, can provide valuable cues for better pose estimation, which however has not been fully exploited. In this paper, we propose a novel Parsing Induced Learner to exploit parsing information to effectively assist pose estimation by learning to fast adapt the base pose estimation model. The proposed Parsing Induced Learner is composed of a parsing encoder and a pose model parameter adapter, which together learn to predict dynamic parameters of the pose model to extract complementary useful features for more accurate pose estimation. Comprehensive experiments on benchmarks LIP and extended PASCAL-Person-Part show that the proposed Parsing Induced Learner can improve performance of both single- and multi-person pose estimation to new state-of-the-art. Cross-dataset experiments also show that the proposed Parsing Induced Learner from LIP dataset can accelerate learning of a human pose estimation model on MPII benchmark in addition to achieving outperforming performance. |</p>

<ul>
<li>The Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation</li>
</ul>

<p>|Abstract|
| - |
| Traditional methods of motion segmentation use powerful geometric constraints to understand motion, but fail to leverage the semantics of high-level image understanding. Modern CNN methods of motion analysis, on the other hand, excel at identifying well-known structures, but may not precisely characterize well-known geometric constraints. In this work, we build a new statistical model of rigid motion flow based on classical perspective projection constraints. We then combine piecewise rigid motions into complex deformable and articulated objects, guided by semantic segmentation from CNNs and a second object-level statistical model. This combination of classical geometric knowledge combined with the pattern recognition abilities of CNNs yields excellent performance on a wide range of motion segmentation benchmarks, from complex geometric scenes to camouflaged animals. |</p>

<hr />

<h1 id="实例分割">实例分割</h1>

<ul>
<li>Deep Extreme Cut: From Extreme Points to Object Segmentation</li>
</ul>

<p>|Abstract|
| - |
| This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points. We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on <a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr">http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr</a>. |</p>

<ul>
<li>Learning to Segment Object Candidates</li>
<li>Learning to Refine Object Segments</li>
</ul>

<hr />

<h1 id="相关资料">相关资料</h1>

<ul>
<li><a href="https://www.zhihu.com/question/51704852">Instance Segmentation 比 Semantic Segmentation 难很多吗？</a></li>
</ul>

        </article>
  </div>
</section>



<aside id="meta">

    <div>
        <section id="datecount">
          <h4 id="date"> Wed Apr 24, 2019 </h4>
          <h5 id="wc"> 900 Words </h5>
          <h5 id="readtime"> Read in about 4 Min </h5>
        </section>
        <ul id="categories">
          
        </ul>
        <ul id="tags">
          
        </ul>
    </div>

    <div>
        <section id="prev">
            &nbsp;
        </section>
        <section id="next">
            &nbsp;<a class="next" href="https://guanfuchen.github.io/post/markdown_blog_ws/markdown_blog_2019_04/panet%E9%98%85%E8%AF%BB/">PANet阅读 <i class="icon-arrow-right"></i></a>
        </section>
    </div>

    

            
            
            
            
            

            
            
                
            
            
            
            
            
            
            
            
            
            
            
            
                
                
            
            
            
            
            
            
            
            
            


        
    
    
    
                
                
            
                                     
                                     
                                     
                                 
                                 
                                 
                                 
                                 
                                 

        

</aside>

<meta itemprop="wordCount" content="830">
<meta itemprop="datePublished" content="2019-04-24">
<meta itemprop="url" content="https://guanfuchen.github.io/post/markdown_blog_ws/markdown_blog_2019_04/cvpr2018_%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%E6%96%87%E7%B1%BB/">


<aside id=comments>
    <div><h2> Comments </h2></div>
    <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "guanfuchen" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</aside>

<footer>
  <div>
    <p>
    &copy; 2017 <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">guanfuchen.</span></span>
    Powered by <a href="http://gohugo.io">Hugo</a>.
  </div>
</footer>
<script type="text/javascript">
(function(){var j=function(a,b){return window.getComputedStyle?getComputedStyle(a).getPropertyValue(b):a.currentStyle[b]};var k=function(a,b,c){if(a.addEventListener)a.addEventListener(b,c,false);else a.attachEvent('on'+b,c)};var l=function(a,b){for(key in b)if(b.hasOwnProperty(key))a[key]=b[key];return a};window.fitText=function(d,e,f){var g=l({'minFontSize':-1/0,'maxFontSize':1/0},f);var h=function(a){var b=e||1;var c=function(){a.style.fontSize=Math.max(Math.min(a.clientWidth/(b*10),parseFloat(g.maxFontSize)),parseFloat(g.minFontSize))+'px'};c();k(window,'resize',c)};if(d.length)for(var i=0;i<d.length;i++)h(d[i]);else h(d);return d}})();
fitText(document.getElementById('title'), 1)
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-7131036-1', 'spf13.com');
  ga('require', 'linkid', 'linkid.js');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');
</script>
</body>
</html>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript"
        src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
